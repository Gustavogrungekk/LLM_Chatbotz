import json
import pandas as pd
import awswrangler as wr  # Certifique-se de ter o awswrangler instalado
from datetime import datetime
from dateutil.relativedelta import relativedelta
import yaml
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from rag_tools.pandas_tools import PandasTool
from rag_tools.documents_tools import DocumentTool
from rag_tools.date_tool import date_tool, DateToolDesc
from rag_tools.more_info_tool import ask_more_info
from util_functions import get_last_chains

class AgentState(dict):
    pass

class MrAgent:
    def __init__(self):
        # Inicializa os atributos do agente
        self.athenas_time = []
        self.load_prompts()
        self.init_model()

    def load_prompts(self):
        # Carrega os prompts a partir de arquivos YAML
        # Exemplo: arquivos 'date_prompt.yaml', 'enrich_prompt.yaml', etc.
        with open("prompts/date_prompt.yaml", "r", encoding="utf-8") as f:
            date_prompt_yaml = yaml.safe_load(f)
        with open("prompts/enrich_prompt.yaml", "r", encoding="utf-8") as f:
            enrich_prompt_yaml = yaml.safe_load(f)
        with open("prompts/mr_camp_prompt.yaml", "r", encoding="utf-8") as f:
            mr_camp_prompt_yaml = yaml.safe_load(f)
        with open("prompts/sugest_pergunta.yaml", "r", encoding="utf-8") as f:
            sugest_prompt_yaml = yaml.safe_load(f)
        with open("prompts/resposta_prompt.yaml", "r", encoding="utf-8") as f:
            resposta_prompt_yaml = yaml.safe_load(f)

        # Converte os prompts para ChatPromptTemplate (adaptando conforme necessário)
        self.date_prompt = ChatPromptTemplate.from_messages(date_prompt_yaml["messages"])
        self.enrich_mr_camp_prompt = ChatPromptTemplate.from_messages(enrich_prompt_yaml["messages"])
        self.mr_camp_prompt = ChatPromptTemplate.from_messages(mr_camp_prompt_yaml["messages"])
        self.suges_pergunta_prompt = ChatPromptTemplate.from_messages(sugest_prompt_yaml["messages"])
        self.resposta_prompt = ChatPromptTemplate.from_messages(resposta_prompt_yaml["messages"])

    def init_model(self):
        # Inicializa as ferramentas e os modelos
        pdt = PandasTool()
        self.pdt = pdt
        self.dt = DocumentTool()
        
        tool_evaluate_pandas_chain = pdt.evaluate_pandas_chain
        self.tool_executor = ToolExecutor([tool_evaluate_pandas_chain])
        self.tools = [tool_evaluate_pandas_chain]
        
        # Ajusta os prompts com informações dos metadados
        self.enrich_mr_camp_prompt = self.enrich_mr_camp_prompt.partial(
            table_description_mr=pdt.get_qstring_mr_camp(),
            column_context_mr= self.dt.get_col_context_mr_camp()
        )
        self.model_enrich_mr_camp = self.enrich_mr_camp_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1)
        
        self.mr_camp_prompt = self.mr_camp_prompt.partial(
            table_description_mr=pdt.get_qstring_mr_camp(),
            column_context_mr= self.dt.get_col_context_mr_camp()
        )
        self.model_mr_camp = self.mr_camp_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1).bind_tools(
            self.tools, parallel_tool_calls=False, tool_choice="evaluate_pandas_chain")
        
        # Configura o extrator de datas
        last_ref = (datetime.strptime(str(max(pdt.get_refs())), "%Y%m") + relativedelta(months=1)).strftime("%Y/%m/%d")
        dates = pdt.get_refs()
        self.date_prompt = self.date_prompt.partial(last_ref=last_ref, datas_disponiveis=dates)
        date_llm = ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1).bind_tools([DateToolDesc], tool_choice='DateToolDesc')
        partial_model = self.date_prompt | date_llm | JsonOutputKeyToolsParser(key_name='DateToolDesc') | (lambda x: x[0]["pandas_str"])
        self.date_extractor = partial_model  # Simplificado para o exemplo
        
        # Configura os modelos de sugestão e verificação
        self.suges_pergunta_prompt = self.suges_pergunta_prompt.partial(
            table_desc=pdt.get_qstring_mr_camp(), metadados=self.dt.get_col_context_mr_camp()
        )
        self.sugest_model = self.suges_pergunta_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1)
        
        self.resposta_prompt = self.resposta_prompt.partial(
            table_desc=pdt.get_qstring_mr_camp(), metadados=self.dt.get_col_context_mr_camp()
        )
        self.resposta_model = self.resposta_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1) \
            .bind_tools([ask_more_info], parallel_tool_calls=False)
        
        self.build_workflow()

    def build_workflow(self):
        # Cria o fluxo de execução do agente
        workflow = StateGraph(AgentState)
        workflow.add_node("date_extraction", self.call_date_extractor)
        workflow.add_node("mr_camp_enrich_agent", self.call_model_mr_camp_enrich)
        workflow.add_node("mr_camp_agent", self.call_model_mr_camp)
        workflow.add_node("mr_camp_action", self.call_tool)
        workflow.add_node("sugest_pergunta", self.call_sugest_pergunta)
        workflow.add_node("add_count", self.add_count)
        workflow.add_node("resposta", self.call_resposta)

        workflow.set_entry_point("date_extraction")
        workflow.add_edge("date_extraction", "mr_camp_enrich_agent")
        workflow.add_edge("mr_camp_enrich_agent", "mr_camp_agent")
        workflow.add_edge("mr_camp_agent", "add_count")
        workflow.add_edge("add_count", "mr_camp_action")
        workflow.add_conditional_edges("mr_camp_action", self.should_ask, {"ask": "sugest_pergunta", "not_ask": "resposta"})
        workflow.add_conditional_edges("resposta", self.need_info, {"more_info": "mr_camp_enrich_agent", "ok": "END"})
        workflow.add_edge("sugest_pergunta", "END")
        self.app = workflow.compile()

    def run(self, context, verbose: bool = True):
        query = context['messages'][-1]["content"]
        memory = context['messages'][:-1]
        inputs = {"messages": [HumanMessage(content=query)], "actions": ["<BEGIN>"], "question": query, "memory": memory, "attempts_count": 0}

        try:
            current_action = []
            inter_list = []

            for output in self.app.stream(inputs, {"recursion_limit": 100}, stream_mode='updates'):
                # Processa os outputs (mantém o debug e log)
                for key, value in output.items():
                    if key.endswith("_action") and verbose:
                        print("Ação atual:", value.get("actions", []))
                    elif key == "date_extraction" and verbose:
                        print("Filtro de data extraído:", value.get("date_filter"))
                    elif key == "sugest_pergunta" and verbose:
                        print("Sugestão de pergunta:", value.get("messages"))
                    elif key == "resposta" and verbose:
                        print("Verificação da resposta:", value.get("messages"))

                    if "actions" in value:
                        current_action.append("->".join(value["actions"][-1]).replace("<BEGIN> -> ", ""))
                    if "inter" in value and value["inter"] is not None:
                        inter_list.append(value["inter"])

                agent_response = output.get("messages", [{"content": ""}])[-1]["content"]
                final_action = current_action[-1] if current_action else ""
                final_table = inter_list[-1] if inter_list else None
                final_message = agent_response.replace('<END>', '').replace('<BEGIN>', '')

            # Aqui, supondo que a query gerada pelo agente esteja contida em final_action ou em outra variável do estado,
            # chamamos run_query para buscar os dados no Athena e substituir o dataframe local.
            # Exemplo:
            if final_action:
                print("Query gerada:", final_action)
                df = self.run_query(final_action)
                # Substituir o dataframe no estado ou enviar para o próximo processamento
                final_table = df

        except Exception as e:
            print("Erro no processamento:", e)
            final_message = "Encontramos um problema processando sua pergunta. Tente novamente, com outra abordagem."
            final_action = ""
            final_table = None

        return final_message, final_action, final_table

    def run_query(self, query: str):
        inicio = datetime.now()
        try:
            df = wr.athena.read_sql_query(
                sql=query,
                database='database_db_compartilhado_consumer_crmcoecampanhaspj',
                workgroup='analytics-workspace-v3',
                ctas_approach=False
            )
            self.athenas_time.append(datetime.now() - inicio)
            print(f"TEMPO EXEC ATHENA: {datetime.now() - inicio}")
            return df
        except Exception as e:
            print("Erro ao executar query no Athena:", e)
            raise

    def call_tool(self, state):
        messages = state['messages']
        last_message = messages[-1]
        output_dict = {"messages": []}

        for idx, tool_call in enumerate(last_message.additional_kwargs.get('tool_calls', [])):
            tool_input = last_message.additional_kwargs['tool_calls'][idx]['function']['arguments']
            tool_input_dict = json.loads(tool_input)
            if last_message.additional_kwargs['tool_calls'][idx]['function']['name'] == 'evaluate_pandas_chain':
                tool_input_dict['inter'] = state.get('inter')
                tool_input_dict['date_filter'] = state.get('date_filter')
                tool_input_dict['agent'] = state.get('agent')
            action = ToolInvocation(
                tool=last_message.additional_kwargs['tool_calls'][idx]['function']['name'],
                tool_input=tool_input_dict
            )
            response, attempted_action, inter = self.tool_executor.invoke(action)
            if "An exception occurred:" in str(response):
                error_info = f"""
                Ações anteriores: {state.get('actions')}
                Ação atual: {attempted_action}
                Result.head(10): {response}
                Query: {state.get('question')}
                Continue a cadeia: action_i -> action_i+1 -> <END>
                """
                function_message = ToolMessage(
                    content=str(error_info),
                    name=action.tool,
                    tool_call_id=tool_call["id"]
                )
                output_dict["messages"].append(function_message)
            else:
                success_info = f"""
                Ações anteriores: {state.get('actions')}
                Ação atual: {attempted_action}
                Result.head(50): {response}
                Query: {state.get('question')}
                Continue a cadeia: action_i -> action_i+1 -> <END>
                """
                function_message = ToolMessage(
                    content=str(success_info),
                    name=action.tool,
                    tool_call_id=tool_call["id"]
                )
                output_dict["messages"].append(function_message)
                output_dict["actions"] = state.get("actions", []) + [attempted_action]
                output_dict["inter"] = inter
                return output_dict

    def call_model_mr_camp_enrich(self, state):
        response = self.model_enrich_mr_camp.invoke(state)
        return {"messages": [response]}

    def call_model_mr_camp(self, state):
        response = self.model_mr_camp.invoke(state)
        return {"messages": [response]}

    def call_date_extractor(self, state):
        date_list = self.date_extractor.invoke(state)
        return {"date_filter": date_list}

    def call_sugest_pergunta(self, state):
        sugestao = self.sugest_model.invoke(state)
        return {"messages": [sugestao]}

    def call_resposta(self, state):
        resposta = self.resposta_model.invoke(state)
        if not resposta.tool_calls:
            return {"messages": [resposta]}
        else:
            resposta = "Mais informações:"
            resposta = AIMessage(resposta)
            return {"messages": [resposta]}

    def should_ask(self, state):
        last_message = state['messages'][-1]
        if (("An exception occurred" in last_message['content']) and (state['attempts_count'] >= 2)) or (state['attempts_count'] >= 4):
            return "ask"
        else:
            return "not_ask"

    def add_count(self, state):
        messages = state['messages']
        last_message = messages[-1]
        if not last_message.get('tool_calls'):
            return {"attempts_count": state['attempts_count']}
        else:
            if last_message['additional_kwargs']['tool_calls'][0]['function']['name'] != 'view_pandas_dataframes':
                qtd_passos = state['attempts_count'] + 1
                return {"attempts_count": qtd_passos}
        return {"attempts_count": state['attempts_count']}

    def need_info(self, state):
        last_message = state['messages'][-1]
        if isinstance(last_message, AIMessage) and last_message.content.startswith("Mais informações:"):
            return "more_info"
        return "ok"
