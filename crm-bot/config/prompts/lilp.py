import json
import pandas as pd
from typing import TypedDict, Annotated, Sequence, List
import operator
from datetime import datetime
from dateutil.relativedelta import relativedelta

# Imports do LangChain e afins
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage, ToolMessage, AIMessage
from langchain_core.utils.function_calling import convert_to_openai_function, convert_to_openai_tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser
from langchain_core.runnables import RunnableParallel
from langchain.agents import Tool

# Imports do LangGraph
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolExecutor, ToolInvocation

# Imports das ferramentas Rag
from rag_tools.pandas_tools import PandasTool
from rag_tools.documents_tools import DocumentTool
from rag_tools.date_tool import date_tool, DateToolDesc
from rag_tools.more_info_tool import ask_more_info

# Importa fun√ß√µes auxiliares
from util_functions import get_last_chains

# Define o tipo do estado
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    actions: Annotated[Sequence[List], operator.add]
    inter: pd.DataFrame
    memory: str
    date_filter: str
    attempts_count: int
    agent: str
    metadados: str
    table_desc: str
    additional_filters: str
    sql_query: str
    df: pd.DataFrame

class MrAgent():
    def __init__(self):
        # Inicializa√ß√£o dos prompts
        self.date_prompt = ChatPromptTemplate.from_messages([
            ("system", """
            Como analista de dados brasileiro especialista em python, sua fun√ß√£o √© extrair as informa√ß√µes relativas a data.
            
            Voc√™ est√° na data: {last_ref}
            
            Sempre forne√ßa a data como um c√≥digo pd.date_range() com o argumento freq 'ME' e no formato 'YYYY-mm-dd'.
            Caso exista apenas a informa√ß√£o m√™s, retorne start-'0000-mm-01' e end-'0000-mm-dd'.
            Caso exista apenas a informa√ß√£o ano, retorne todo o intervalo do ano.
            Caso n√£o exista informa√ß√£o de data, retorne pd.date_range(start='0000-00-00', end='0000-00-00', freq='ME').
            Caso a pergunta contenha a express√£o "m√™s a m√™s" ou "refer√™ncia", retorne pd.date_range(start='3333-00-00', end='3333-00-00', freq='ME').
            Caso a pergunta contenha "√∫ltimo(s) m√™s(es)", retorne os √∫ltimos meses de acordo com a pergunta.
            
            Nunca forne√ßa intervalos de data maiores que fevereiro de 2025.
            """),
            MessagesPlaceholder(variable_name="memory"),
            ("user", "(question)")
        ])

        self.enrich_mr_camp_str = """
        Como engenheiro de prompt, sua fun√ß√£o √© reescrever e detalhar a pergunta de forma que um modelo de LLM consiga responder.
        
        Considere que voc√™ tem acesso a seguinte tabela para enriquecer a resposta:
        {table_description_mr}
        {column_context_mr}
        
        Pergunta do usu√°rio:
        {question}
        
        Reescreva de forma sucinta a pergunta indicando quais filtros s√£o necess√°rios realizar para respond√™-la.
        Atente-se √† pergunta! N√£o infira nada al√©m do que est√° nela.
        
        Caso a pergunta contenha algum conceito que n√£o est√° nos metadados, redija a pergunta de forma a dizer que n√£o consegue responder.
        
        Considere que a pergunta possui o seguinte filtro na coluna 'safra': {date_filter}
        """
        self.enrich_mr_camp_prompt = ChatPromptTemplate.from_messages([
            ("system", self.enrich_mr_camp_str),
            MessagesPlaceholder(variable_name="memory"),
            MessagesPlaceholder(variable_name="messages")
        ])

        self.sql_gen_prompt_str = """
        Como especialista em AWS Athena e an√°lise de dados banc√°rios, sua fun√ß√£o √© criar queries SQL eficientes seguindo estas regras:
        **Tabela e Parti√ß√µes:**
        - Nome da tabela: `tbl_coeres_painel_anomes_v1_gold`
        - Parti√ß√µes obrigat√≥rias (sempre filtrar como strings):
          - `year` (formato 'YYYY', ex: '2025')
          - `month` (formato 'MM', ex: '01')
          - `canal` (valores como 'VAI', 'APP', etc.)

        **Colunas Dispon√≠veis:**
        1. `cnpj` (bigint): Identificador √∫nico do cliente
        2. `produto` (string): Tipo de produto financeiro (ex: cart√£o, empr√©stimo)
        3. `abastecido` (int): 0 ou 1 - cliente recebeu campanha
        4. `potencial` (int): 0 ou 1 - cliente √© potencial
        5. `visto` (int): 0 ou 1 - cliente visualizou a campanha
        6. `clique` (int): 0 ou 1 - cliente clicou na campanha
        7. `atuado` (int): 0 ou 1 - cliente foi atuado
        8. `disponivel` (int): 0 ou 1 - campanha dispon√≠vel
        9. `metrica_engajamento` (double): √çndice de engajamento

        **Regras Obrigat√≥rias:**
        1. SEMPRE inclua filtros para `year`, `month` e `canal` como strings (ex: `year = '2025'`).
        2. Use `COUNT(DISTINCT CASE WHEN coluna = 1 THEN cnpj END)` para m√©tricas bin√°rias.
        3. Selecione explicitamente as colunas necess√°rias - NUNCA use `SELECT *`.
        4. Valide datas: `year` com 4 d√≠gitos, `month` com 2 d√≠gitos.
        5. Priorize performance com `LIMIT` e filtros adequados.

        **Exemplo de Query V√°lida:**
        ```sql
        SELECT 
          canal,
          COUNT(DISTINCT CASE WHEN potencial = 1 THEN cnpj END) AS clientes_potenciais,
          COUNT(DISTINCT CASE WHEN visto = 1 THEN cnpj END) AS visualizacoes
        FROM tbl_coeres_painel_anomes_v1_gold
        WHERE year = '2025' 
          AND month = '01'
          AND canal = 'VAI'
        GROUP BY canal
        LIMIT 1000
        ```
        """
        self.sql_gen_prompt = ChatPromptTemplate.from_messages([
            ("system", self.sql_gen_prompt_str),
            ("user", "Pergunta: {question}\nMetadados: {metadados}")
        ])

        self.mr_camp_prompt_str = """
        Como engenheiro de dados brasileiro, especializado em an√°lise de dados banc√°rios e engajamento do cliente, seu papel √© responder exclusivamente a perguntas sobre a M√°quina de Resultados, um conjunto de dados utilizado para acompanhar o desempenho de campanhas e a√ß√µes de CRM.
        
        Voc√™ tem acesso ao dataframe 'df' obtido atrav√©s da execu√ß√£o da query SQL no Athena. Esta query foi gerada dinamicamente com base nos metadados e na pergunta do usu√°rio.
        
        Baseando-se nas descri√ß√µes das colunas dispon√≠veis:
        {column_context_mr}
        
        Identifique quais colunas est√£o diretamente relacionadas com a pergunta feita no chat. Ap√≥s essa identifica√ß√£o, desenvolva e execute uma sequ√™ncia de comandos utilizando a ferramenta 'evaluate_pandas_chain', estruturando-os da seguinte maneira:
        
        <BEGIN> -> action1 -> action2 -> action3 -> <END>.
        
        Atente-se:
        - Use str.contains() para buscar valores em colunas de string.
        - Os valores em colunas string est√£o em CAPSLOCK.
        - Se a pergunta contiver conceitos n√£o presentes nos metadados, n√£o infira uma resposta.
        - Retorne uma tabela em markdown quando solicitado.
        """
        self.mr_camp_prompt = ChatPromptTemplate.from_messages([
            ("system", self.mr_camp_prompt_str),
            MessagesPlaceholder(variable_name="messages", n_message=1)
        ])

        self.suges_pergunta_prompt_desc = """
        Voc√™ √© um assistente de IA especializado em melhorar a clareza das perguntas dos usu√°rios.
        
        Analise a pergunta original para identificar se h√° informa√ß√µes faltantes ou ambiguidades. Caso necess√°rio, questione o usu√°rio para obter os detalhes que permitam uma resposta completa.
        
        Considere que h√° um dataframe com as seguintes colunas: {metadados} e descri√ß√£o: {table_desc}.
        
        Se a pergunta estiver clara, confirme o entendimento; se n√£o, pe√ßa esclarecimentos.
        """
        self.suges_pergunta_prompt = ChatPromptTemplate.from_messages([
            ("system", self.suges_pergunta_prompt_desc),
            MessagesPlaceholder(variable_name="memory"),
            ("user", "(question)")
        ])

        self.resposta_prompt_desc = """
        Voc√™ √© um analista de dados especializado em dados banc√°rios e engajamento.
        
        Verifique se as respostas t√©cnicas fornecidas cont√™m todas as informa√ß√µes necess√°rias para responder √† pergunta do usu√°rio.
        
        Utilize as informa√ß√µes do dataframe (descri√ß√£o: {table_desc} e metadados: {metadados}) para validar a resposta.
        
        Se as informa√ß√µes estiverem completas, formate a resposta; se n√£o, indique que faltam dados e solicite mais informa√ß√µes.
        """
        self.resposta_prompt = ChatPromptTemplate.from_messages([
            ("system", self.resposta_prompt_desc),
            MessagesPlaceholder(variable_name="memory"),
            MessagesPlaceholder(variable_name="messages")
        ])

        # Inicializa os modelos e ferramentas
        self.init_model()

    # M√âTODOS DOS N√ìS DO WORKFLOW (todos retornam PromptValues ou strings)

    def call_date_extractor(self, state):
        date_list = self.date_extractor.invoke(state)
        if isinstance(date_list, list):
            return " ".join([msg.content for msg in date_list])
        return str(date_list)

    def call_model_mr_camp_enrich(self, state):
        response = self.model_enrich_mr_camp.invoke(state)
        return [response]

    def call_sql_generator(self, state):
        response = self.sql_gen_model.invoke(state)
        return response.content.strip()

    def call_run_athena_query(self, state):
        query = state.get("sql_query", "")
        if query:
            df = self.run_query(query)
            state["df"] = df
        return ""

    def call_model_mr_camp(self, state):
        response = self.model_mr_camp.invoke(state)
        return [response]

    def call_sugest_pergunta(self, state):
        sugestao = self.sugest_model.invoke(state)
        return [sugestao]

    def call_resposta(self, state):
        resposta = self.resposta_model.invoke(state)
        if not resposta.tool_calls:
            return [resposta]
        else:
            return [AIMessage(content="Mais informa√ß√µes:")]

    def call_tool(self, state):
        last_message = state["messages"][-1]
        if "tool_calls" in last_message.additional_kwargs:
            for tc in last_message.additional_kwargs["tool_calls"]:
                tool_input = tc["function"]["arguments"]
                tool_input_dict = json.loads(tool_input)
                action = ToolInvocation(
                    tool=tc["function"]["name"],
                    tool_input=tool_input_dict
                )
                response, attempted_action, inter = self.tool_executor.invoke(action)
                summary = f"Tool '{action.tool}' executada com a√ß√£o: {attempted_action}. Resultado: {response}"
                return summary
        return "Nenhuma ferramenta executada."

    def end_node(self, state):
        return [AIMessage(content="Fim do workflow.")]

    # M√âTODO RUN: Executa o workflow
    def run(self, context, verbose: bool = True):
        print("Streamlit session state:")
        print(context)
        query = context['messages'][-1]["content"]
        memory = context['messages'][:-1]
        # Estado inicial
        state = {
            "messages": [HumanMessage(content=query)],
            "actions": ["<BEGIN>"],
            "question": query,
            "memory": memory,
            "attempts_count": 0
        }
        try:
            # Aqui simulamos o streaming do workflow
            for output in self.app.stream(state, {"recursion_limit": 100}, stream_mode='updates'):
                print("Output:", output)
            # Ao final, pegamos o √∫ltimo output
            final_output = self.app.run(state)
            # Supomos que final_output seja uma lista de mensagens
            final_message = " ".join([msg.content for msg in final_output])
        except Exception as e:
            print("Houve um erro no processo:")
            print(e)
            final_message = "Encontramos um problema processando sua pergunta. Tente novamente, com outra abordagem."
        return final_message

    # M√âTODO init_model: Configura modelos, ferramentas e workflow
    def init_model(self):
        pdt = PandasTool()
        self.pdt = pdt
        tool_evaluate_pandas_chain = pdt.evaluate_pandas_chain

        dt = DocumentTool()
        self.dt = dt

        tools = [tool_evaluate_pandas_chain]
        self.tool_executor = ToolExecutor(tools)
        self.tools = [convert_to_openai_tool(t) for t in tools]

        self.enrich_mr_camp_prompt = self.enrich_mr_camp_prompt.partial(
            table_description_mr=pdt.get_qstring_mr_camp()
        ).partial(
            column_context_mr=dt.get_col_context_mr_camp()
        )
        self.model_enrich_mr_camp = self.enrich_mr_camp_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1)

        self.sql_gen_prompt = self.sql_gen_prompt.partial(metadados=dt.get_col_context_mr_camp())
        self.sql_gen_model = self.sql_gen_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1)

        self.mr_camp_prompt = self.mr_camp_prompt.partial(column_context_mr=dt.get_col_context_mr_camp())
        self.model_mr_camp = self.mr_camp_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1).bind_tools(
            self.tools, parallel_tool_calls=False, tool_choice="evaluate_pandas_chain"
        )

        last_ref = (datetime.strptime(str(max(pdt.get_refs())), "%Y%m") + relativedelta(months=1)).strftime("%Y/%m/%d")
        dates = pdt.get_refs()
        self.date_prompt = self.date_prompt.partial(last_ref=last_ref).partial(datas_disponiveis=dates)
        date_llm = ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1).bind_tools(
            [DateToolDesc], tool_choice='DateToolDesc'
        )
        partial_model = self.date_prompt | date_llm | JsonOutputKeyToolsParser(key_name='DateToolDesc') | (lambda x: x[0]["pandas_str"])
        self.date_extractor = RunnableParallel(pandas_str=partial_model, refs_list=lambda x: pdt.get_refs()) | ChatOpenAI(
            model="gpt-4-0125-preview", temperature=0, seed=1
        )

        self.suges_pergunta_prompt = self.suges_pergunta_prompt.partial(
            table_desc=pdt.get_qstring_mr_camp()
        ).partial(
            metadados=dt.get_col_context_mr_camp()
        )
        self.sugest_model = self.suges_pergunta_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1)

        self.resposta_prompt = self.resposta_prompt.partial(
            table_desc=pdt.get_qstring_mr_camp()
        ).partial(
            metadados=dt.get_col_context_mr_camp()
        )
        self.resposta_model = self.resposta_prompt | ChatOpenAI(model="gpt-4-0125-preview", temperature=0, seed=1).bind_tools(
            [ask_more_info], parallel_tool_calls=False
        )

        self.build_workflow()

    def build_workflow(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("date_extraction", self.call_date_extractor)
        workflow.add_node("mr_camp_enrich_agent", self.call_model_mr_camp_enrich)
        workflow.add_node("sql_generator", self.call_sql_generator)
        workflow.add_node("run_athena_query", self.call_run_athena_query)
        workflow.add_node("mr_camp_agent", self.call_model_mr_camp)
        workflow.add_node("add_count", self.add_count)
        workflow.add_node("mr_camp_action", self.call_tool)
        workflow.add_node("sugest_pergunta", self.call_sugest_pergunta)
        workflow.add_node("resposta", self.call_resposta)
        workflow.add_node("END", self.end_node)

        workflow.set_entry_point("date_extraction")
        workflow.add_edge("date_extraction", "mr_camp_enrich_agent")
        workflow.add_edge("mr_camp_enrich_agent", "sql_generator")
        workflow.add_edge("sql_generator", "run_athena_query")
        workflow.add_edge("run_athena_query", "mr_camp_agent")
        workflow.add_edge("mr_camp_agent", "add_count")
        workflow.add_edge("add_count", "mr_camp_action")
        workflow.add_conditional_edges("mr_camp_action", self.should_ask, {"ask": "sugest_pergunta", "not_ask": "resposta"})
        workflow.add_conditional_edges("resposta", self.need_info, {"more_info": "mr_camp_enrich_agent", "ok": "END"})
        workflow.add_edge("sugest_pergunta", "END")
        self.app = workflow.compile()




def run(self, context, verbose: bool = True):
    print("Streamlit session state:")
    print(context)

    # Obt√©m a √∫ltima pergunta e a mem√≥ria
    if isinstance(context, list):
        query = context[-1].content  # Acessa o conte√∫do diretamente, sem subscri√ß√£o
        memory = context[:-1]  # As mensagens anteriores
    else:
        query = context.get('messages', [])[-1].content  # Acessa o conte√∫do corretamente
        memory = context.get('messages', [])[:-1]  # Pega as mensagens anteriores

    # Estado inicial
    state = {
        "messages": [HumanMessage(content=query)],  # √öltima mensagem
        "actions": ["<BEGIN>"],
        "question": query,
        "memory": [HumanMessage(content=m.content) for m in memory if m],  # Mensagens anteriores
        "attempts_count": 0
    }

    # üîç DEBUG: Printar o estado antes de rodar stream()
    print("===== DEBUG: Estado antes de rodar stream =====")
    for key, value in state.items():
        print(f"{key}: {type(value)} -> {value}")
    print("================================================")

    try:
        # Simula√ß√£o de fluxo
        max_iter = 10  # Evita loop infinito
        count = 0

        for output in self.app.stream(state, {"recursion_limit": 100}, stream_mode='updates'):
            print("Output:", output)
            count += 1
            if count >= max_iter:
                print("üö® Loop interrompido para evitar travamento!")
                break

        final_output = self.app.run(state["messages"])  # Passa apenas as mensagens
        final_message = " ".join([msg.content for msg in final_output])

    except Exception as e:
        print("Houve um erro no processo:", e)
        final_message = "Encontramos um problema processando sua pergunta. Tente novamente."

    return final_message
